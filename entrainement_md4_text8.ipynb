{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okqt58jwegbX"
      },
      "source": [
        "# Entraînement d'un modèle MD4 sur text8\n",
        "\n",
        "Ce notebook a pour but d'entraîner un modèle du dépôt `md4` sur le jeu de données `text8`.\n",
        "\n",
        "**Objectifs :**\n",
        "- Télécharger et préparer le jeu de données `text8`.\n",
        "- Configurer un modèle avec moins de 25 millions de paramètres.\n",
        "- Utiliser au maximum le code du dépôt fourni.\n",
        "- Lancer une boucle d'entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F_7G3M7egbZ"
      },
      "source": [
        "## 1. Installation des dépendances\n",
        "\n",
        "Nous commençons par installer les bibliothèques nécessaires listées dans `requirements_gpu.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD8IwYqLegbZ",
        "outputId": "bbd5081d-530e-406e-a135-27120dd749c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clu\n",
            "  Downloading clu-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting distrax\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting grain\n",
            "  Downloading grain-0.2.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (0.10.6)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from clu) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from clu) (1.12.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from clu) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from clu) (0.5.1)\n",
            "Collecting ml-collections (from clu)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from clu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clu) (24.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from clu) (4.14.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from clu) (1.17.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.1.89)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.25.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.11/dist-packages (from grain) (0.7.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from grain) (3.1.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from grain) (0.1.9)\n",
            "Requirement already satisfied: more-itertools>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from grain) (10.7.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from grain) (5.29.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.11.16)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (0.12.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->clu) (1.15.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Downloading clu-0.0.12-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grain-0.2.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.5/485.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-collections, grain, distrax, clu\n",
            "Successfully installed clu-0.0.12 distrax-0.1.5 grain-0.2.10 ml-collections-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install clu datasets distrax grain matplotlib seaborn tensorflow tensorflow-datasets tf-keras transformers flax optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rDphwbSegba"
      },
      "source": [
        "## 2. Imports\n",
        "\n",
        "Importation des modules nécessaires depuis le dépôt `md4` et d'autres bibliothèques."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fais une cellule qui se connecte a driive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E6T9xI_exBy",
        "outputId": "bb91f40c-3ac7-4378-ad66-67f725d7faa9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "exV7mYR6egba"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from collections.abc import Mapping\n",
        "import functools\n",
        "import copy\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from ml_collections import config_dict\n",
        "import tensorflow as tf\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "from clu import parameter_overview\n",
        "\n",
        "# Supposons que le dépôt md4 est dans le répertoire courant\n",
        "# Si ce n'est pas le cas, ajoutez le chemin au PYTHONPATH\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/Stage3A/travail/md4-main')\n",
        "\n",
        "from md4 import input_pipeline\n",
        "from md4.models import utils as model_utils\n",
        "from md4 import train as train_lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4lCaNdZegba"
      },
      "source": [
        "## 3. Préparation du jeu de données text8\n",
        "\n",
        "Cette section s'occupe du téléchargement et de la préparation du jeu de données `text8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3B87tMRegba",
        "outputId": "729615f3-2cf6-47db-80e9-12d00164c295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Téléchargement de text8 depuis http://mattmahoney.net/dc/text8.zip...\n",
            "Téléchargement terminé.\n",
            "Fichiers de données text8 créés.\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = './text8_data'\n",
        "\n",
        "def preprocess_text8(data_dir):\n",
        "    \"\"\"Télécharge et extrait le jeu de données text8.\"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    zip_path = os.path.join(data_dir, 'text8.zip')\n",
        "    if not os.path.exists(zip_path):\n",
        "        url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "        print(f'Téléchargement de text8 depuis {url}...')\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "        print('Téléchargement terminé.')\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as f:\n",
        "        rawdata = f.read('text8').decode('utf-8')\n",
        "\n",
        "    # Créer les fichiers de split\n",
        "    splits = {\n",
        "        'train': rawdata[:90000000],\n",
        "        'valid': rawdata[90000000:95000000],\n",
        "        'test': rawdata[95000000:],\n",
        "    }\n",
        "    for split, data in splits.items():\n",
        "        with open(os.path.join(data_dir, f'text8.{split}.txt'), 'w') as f_out:\n",
        "            f_out.write(data)\n",
        "    print('Fichiers de données text8 créés.')\n",
        "    return splits\n",
        "\n",
        "text8_splits = preprocess_text8(DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mr_919cegbb"
      },
      "source": [
        "## 4. Configuration\n",
        "\n",
        "Définition de la configuration pour le modèle et l'entraînement. Les paramètres sont ajustés pour rester sous la barre des 25M de paramètres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JcK6o1GUegbb"
      },
      "outputs": [],
      "source": [
        "def get_text8_config():\n",
        "    config = config_dict.ConfigDict()\n",
        "\n",
        "    # Dataset\n",
        "    config.dataset = 'text8'\n",
        "    config.data_shape = (256,)\n",
        "    config.vocab_size = 27\n",
        "    config.classes = -1\n",
        "\n",
        "    # Model\n",
        "    config.task_type = 'text'\n",
        "    config.model_type = 'md4'\n",
        "    config.timesteps = 1000\n",
        "    config.noise_schedule = 'linear'\n",
        "    config.outside_embed = True\n",
        "    config.time_features = 't'\n",
        "    config.cont_time = True\n",
        "\n",
        "    # --- Paramètres ajustés pour < 25M de paramètres ---\n",
        "    config.feature_dim = 64  # Diminuer la dimension des caractéristiques\n",
        "    config.n_layers = 8     # Diminuer le nombre de couches\n",
        "    config.num_heads = 6   # Diminuer le nombre de têtes d'attention\n",
        "    # -----------------------------------------------------\n",
        "\n",
        "    config.mlp_type = 'glu'\n",
        "    config.depth_scaled_init = True\n",
        "    config.cond_type = 'adaln_zero'\n",
        "    config.n_dit_layers = 0\n",
        "    config.dit_num_heads = 12\n",
        "    config.dit_hidden_size = 768\n",
        "    config.ch_mult = (1,)\n",
        "    config.dropout_rate = 0 # 0.05\n",
        "\n",
        "    # Training\n",
        "    config.learning_rate = 3e-4 # 3e-4\n",
        "    config.learning_rate_schedule = 'cosine'\n",
        "    config.warmup_steps = 2000\n",
        "    config.weight_decay = 0.03 # 0.03\n",
        "    config.clip = 1.0\n",
        "    config.b2 = 0.999\n",
        "    config.num_epochs = 1 # Pour la démo\n",
        "    config.ema_rate = 0.9999\n",
        "    config.num_train_steps = 50_000 # Pour la démo\n",
        "    config.batch_size = 256 # 128\n",
        "    config.num_microbatches = 2\n",
        "    config.check_nans = False\n",
        "\n",
        "    # Logging & Checkpointing\n",
        "    config.log_loss_every_steps = 100\n",
        "    config.eval_every_steps = 2500\n",
        "    config.checkpoint_every_steps = 10000\n",
        "    config.checkpoint_keep_period = 5000\n",
        "\n",
        "    # Sampling\n",
        "    config.sampler = 'ancestral'\n",
        "    config.sampling_grid = 'cosine'\n",
        "    config.topp = 0.98\n",
        "\n",
        "    # Misc\n",
        "    config.seed = 42\n",
        "    config.grain_num_workers = 8 # 2\n",
        "\n",
        "    return config\n",
        "\n",
        "config = get_text8_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIpbm2ZFegbb"
      },
      "source": [
        "## 5. Création du modèle et vérification des paramètres\n",
        "\n",
        "Nous créons le pipeline de données, le modèle, et nous nous assurons qu'il respecte la contrainte de taille."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw_TMgdWegbb",
        "outputId": "b75eed7f-e8dd-4054-8717-1273fe199085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres du modèle : 15.47M\n",
            "\u001b[92mLe nombre de paramètres est bien inférieur à 25 millions.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Création du pipeline de données\n",
        "tokenizer = input_pipeline.Text8Tokenizer()\n",
        "train_source = input_pipeline.ChunkDataSource(text8_splits['train'], chunk_size=config.data_shape[0], overlapping=True)\n",
        "train_loader = input_pipeline.grain.load(\n",
        "    source=train_source,\n",
        "    shuffle=True,\n",
        "    seed=config.seed,\n",
        "    shard_options=input_pipeline.grain.ShardByJaxProcess(drop_remainder=True),\n",
        "    transformations=[input_pipeline.Tokenize(tokenizer)],\n",
        "    batch_size=config.batch_size // jax.process_count(),\n",
        "    worker_count=config.grain_num_workers,\n",
        ")\n",
        "\n",
        "rng = jax.random.PRNGKey(config.seed)\n",
        "rng, model_rng = jax.random.split(rng)\n",
        "\n",
        "# Création du modèle\n",
        "model = model_utils.get_model(config)\n",
        "\n",
        "# Initialisation du modèle pour compter les paramètres\n",
        "dummy_input = jnp.ones((1,) + config.data_shape, dtype=jnp.int32)\n",
        "params = model.init(model_rng, dummy_input, train=False)['params']\n",
        "\n",
        "# Calcul et affichage du nombre de paramètres\n",
        "num_params = parameter_overview.count_parameters(params)\n",
        "print(f\"Nombre total de paramètres du modèle : {num_params / 1e6:.2f}M\")\n",
        "\n",
        "if num_params > 25_000_000:\n",
        "    print(\"\\033[91mAttention : Le nombre de paramètres dépasse 25 millions !\\033[0m\")\n",
        "else:\n",
        "    print(\"\\033[92mLe nombre de paramètres est bien inférieur à 25 millions.\\033[0m\")\n",
        "\n",
        "parameter_overview.log_parameter_overview(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjuwtU-Qegbb"
      },
      "source": [
        "## 6. Entraînement\n",
        "\n",
        "Mise en place de la boucle d'entraînement et exécution pour quelques étapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htSQA7rmegbc",
        "outputId": "aa9e3f70-c3bb-4884-d433-1c6d2d8a2dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0/50000 - Loss: 4.8538 - LR: 0.000000\n",
            "Step 100/50000 - Loss: 4.4552 - LR: 0.000015\n",
            "Step 200/50000 - Loss: 4.1644 - LR: 0.000030\n",
            "Step 300/50000 - Loss: 4.0744 - LR: 0.000045\n",
            "Step 400/50000 - Loss: 3.8024 - LR: 0.000060\n",
            "Step 500/50000 - Loss: 3.6492 - LR: 0.000075\n",
            "Step 600/50000 - Loss: 3.3597 - LR: 0.000090\n",
            "Step 700/50000 - Loss: 3.2041 - LR: 0.000105\n",
            "Step 800/50000 - Loss: 3.1269 - LR: 0.000120\n",
            "Step 900/50000 - Loss: 2.9522 - LR: 0.000135\n",
            "Step 1000/50000 - Loss: 2.9251 - LR: 0.000150\n",
            "Step 1100/50000 - Loss: 2.8924 - LR: 0.000165\n",
            "Step 1200/50000 - Loss: 2.8126 - LR: 0.000180\n",
            "Step 1300/50000 - Loss: 2.7305 - LR: 0.000195\n",
            "Step 1400/50000 - Loss: 2.7652 - LR: 0.000210\n",
            "Step 1500/50000 - Loss: 2.6767 - LR: 0.000225\n",
            "Step 1600/50000 - Loss: 2.7269 - LR: 0.000240\n",
            "Step 1700/50000 - Loss: 2.6738 - LR: 0.000255\n",
            "Step 1800/50000 - Loss: 2.5846 - LR: 0.000270\n",
            "Step 1900/50000 - Loss: 2.6327 - LR: 0.000285\n",
            "Step 2000/50000 - Loss: 2.5790 - LR: 0.000300\n",
            "Step 2100/50000 - Loss: 2.5070 - LR: 0.000300\n",
            "Step 2200/50000 - Loss: 2.5140 - LR: 0.000300\n",
            "Step 2300/50000 - Loss: 2.5069 - LR: 0.000300\n",
            "Step 2400/50000 - Loss: 2.5309 - LR: 0.000300\n",
            "Step 2500/50000 - Loss: 2.4814 - LR: 0.000300\n",
            "Step 2600/50000 - Loss: 2.4124 - LR: 0.000300\n",
            "Step 2700/50000 - Loss: 2.4110 - LR: 0.000300\n",
            "Step 2800/50000 - Loss: 2.4024 - LR: 0.000300\n",
            "Step 2900/50000 - Loss: 2.4330 - LR: 0.000300\n",
            "Step 3000/50000 - Loss: 2.4279 - LR: 0.000300\n",
            "Step 3100/50000 - Loss: 2.3353 - LR: 0.000300\n",
            "Step 3200/50000 - Loss: 2.3079 - LR: 0.000300\n",
            "Step 3300/50000 - Loss: 2.4734 - LR: 0.000299\n",
            "Step 3400/50000 - Loss: 2.3511 - LR: 0.000299\n",
            "Step 3500/50000 - Loss: 2.3433 - LR: 0.000299\n",
            "Step 3600/50000 - Loss: 2.3376 - LR: 0.000299\n",
            "Step 3700/50000 - Loss: 2.3072 - LR: 0.000299\n",
            "Step 3800/50000 - Loss: 2.3880 - LR: 0.000299\n",
            "Step 3900/50000 - Loss: 2.2946 - LR: 0.000299\n",
            "Step 4000/50000 - Loss: 2.3149 - LR: 0.000299\n",
            "Step 4100/50000 - Loss: 2.3007 - LR: 0.000299\n",
            "Step 4200/50000 - Loss: 2.2921 - LR: 0.000298\n",
            "Step 4300/50000 - Loss: 2.4453 - LR: 0.000298\n",
            "Step 4400/50000 - Loss: 2.3016 - LR: 0.000298\n",
            "Step 4500/50000 - Loss: 2.2858 - LR: 0.000298\n",
            "Step 4600/50000 - Loss: 2.2745 - LR: 0.000298\n",
            "Step 4700/50000 - Loss: 2.2391 - LR: 0.000298\n",
            "Step 4800/50000 - Loss: 2.2474 - LR: 0.000297\n",
            "Step 4900/50000 - Loss: 2.2931 - LR: 0.000297\n",
            "Step 5000/50000 - Loss: 2.2133 - LR: 0.000297\n",
            "Step 5100/50000 - Loss: 2.2637 - LR: 0.000297\n",
            "Step 5200/50000 - Loss: 2.2585 - LR: 0.000297\n",
            "Step 5300/50000 - Loss: 2.2348 - LR: 0.000297\n",
            "Step 5400/50000 - Loss: 2.2945 - LR: 0.000296\n",
            "Step 5500/50000 - Loss: 2.2610 - LR: 0.000296\n",
            "Step 5600/50000 - Loss: 2.2352 - LR: 0.000296\n",
            "Step 5700/50000 - Loss: 2.2191 - LR: 0.000296\n",
            "Step 5800/50000 - Loss: 2.1995 - LR: 0.000295\n",
            "Step 5900/50000 - Loss: 2.2393 - LR: 0.000295\n",
            "Step 6000/50000 - Loss: 2.2259 - LR: 0.000295\n",
            "Step 6100/50000 - Loss: 2.2108 - LR: 0.000295\n",
            "Step 6200/50000 - Loss: 2.2125 - LR: 0.000294\n",
            "Step 6300/50000 - Loss: 2.2387 - LR: 0.000294\n",
            "Step 6400/50000 - Loss: 2.2565 - LR: 0.000294\n",
            "Step 6500/50000 - Loss: 2.1823 - LR: 0.000294\n",
            "Step 6600/50000 - Loss: 2.1591 - LR: 0.000293\n",
            "Step 6700/50000 - Loss: 2.1873 - LR: 0.000293\n",
            "Step 6800/50000 - Loss: 2.1784 - LR: 0.000293\n",
            "Step 6900/50000 - Loss: 2.1397 - LR: 0.000292\n",
            "Step 7000/50000 - Loss: 2.1687 - LR: 0.000292\n",
            "Step 7100/50000 - Loss: 2.1577 - LR: 0.000292\n",
            "Step 7200/50000 - Loss: 2.2052 - LR: 0.000291\n",
            "Step 7300/50000 - Loss: 2.1424 - LR: 0.000291\n",
            "Step 7400/50000 - Loss: 2.2017 - LR: 0.000291\n",
            "Step 7500/50000 - Loss: 2.1651 - LR: 0.000290\n",
            "Step 7600/50000 - Loss: 2.2134 - LR: 0.000290\n",
            "Step 7700/50000 - Loss: 2.1809 - LR: 0.000290\n",
            "Step 7800/50000 - Loss: 2.1659 - LR: 0.000289\n",
            "Step 7900/50000 - Loss: 2.1853 - LR: 0.000289\n",
            "Step 8000/50000 - Loss: 2.1218 - LR: 0.000289\n",
            "Step 8100/50000 - Loss: 2.1531 - LR: 0.000288\n",
            "Step 8200/50000 - Loss: 2.1416 - LR: 0.000288\n",
            "Step 8300/50000 - Loss: 2.1344 - LR: 0.000287\n",
            "Step 8400/50000 - Loss: 2.1453 - LR: 0.000287\n",
            "Step 8500/50000 - Loss: 2.1697 - LR: 0.000287\n",
            "Step 8600/50000 - Loss: 2.1672 - LR: 0.000286\n",
            "Step 8700/50000 - Loss: 2.2128 - LR: 0.000286\n",
            "Step 8800/50000 - Loss: 2.1307 - LR: 0.000285\n",
            "Step 8900/50000 - Loss: 2.1242 - LR: 0.000285\n",
            "Step 9000/50000 - Loss: 2.1181 - LR: 0.000285\n",
            "Step 9100/50000 - Loss: 2.1676 - LR: 0.000284\n",
            "Step 9200/50000 - Loss: 2.1163 - LR: 0.000284\n",
            "Step 9300/50000 - Loss: 2.1239 - LR: 0.000283\n",
            "Step 9400/50000 - Loss: 2.0641 - LR: 0.000283\n",
            "Step 9500/50000 - Loss: 2.0962 - LR: 0.000282\n",
            "Step 9600/50000 - Loss: 2.1014 - LR: 0.000282\n",
            "Step 9700/50000 - Loss: 2.0789 - LR: 0.000281\n",
            "Step 9800/50000 - Loss: 2.0936 - LR: 0.000281\n",
            "Step 9900/50000 - Loss: 2.1271 - LR: 0.000280\n",
            "Step 10000/50000 - Loss: 2.1032 - LR: 0.000280\n",
            "Step 10100/50000 - Loss: 2.0919 - LR: 0.000279\n",
            "Step 10200/50000 - Loss: 2.1081 - LR: 0.000279\n",
            "Step 10300/50000 - Loss: 2.1000 - LR: 0.000278\n",
            "Step 10400/50000 - Loss: 2.0447 - LR: 0.000278\n",
            "Step 10500/50000 - Loss: 2.1789 - LR: 0.000277\n",
            "Step 10600/50000 - Loss: 2.1272 - LR: 0.000277\n",
            "Step 10700/50000 - Loss: 2.0766 - LR: 0.000276\n",
            "Step 10800/50000 - Loss: 2.0506 - LR: 0.000276\n",
            "Step 10900/50000 - Loss: 2.0630 - LR: 0.000275\n",
            "Step 11000/50000 - Loss: 2.1206 - LR: 0.000275\n",
            "Step 11100/50000 - Loss: 2.0753 - LR: 0.000274\n",
            "Step 11200/50000 - Loss: 2.1157 - LR: 0.000274\n",
            "Step 11300/50000 - Loss: 2.1284 - LR: 0.000273\n",
            "Step 11400/50000 - Loss: 2.1765 - LR: 0.000272\n",
            "Step 11500/50000 - Loss: 2.1646 - LR: 0.000272\n",
            "Step 11600/50000 - Loss: 2.1546 - LR: 0.000271\n",
            "Step 11700/50000 - Loss: 2.0811 - LR: 0.000271\n",
            "Step 11800/50000 - Loss: 2.0535 - LR: 0.000270\n",
            "Step 11900/50000 - Loss: 2.0657 - LR: 0.000270\n",
            "Step 12000/50000 - Loss: 2.1098 - LR: 0.000269\n",
            "Step 12100/50000 - Loss: 2.0598 - LR: 0.000268\n",
            "Step 12200/50000 - Loss: 2.1263 - LR: 0.000268\n",
            "Step 12300/50000 - Loss: 2.0751 - LR: 0.000267\n",
            "Step 12400/50000 - Loss: 2.1155 - LR: 0.000267\n",
            "Step 12500/50000 - Loss: 2.0907 - LR: 0.000266\n",
            "Step 12600/50000 - Loss: 2.0668 - LR: 0.000265\n",
            "Step 12700/50000 - Loss: 2.0981 - LR: 0.000265\n",
            "Step 12800/50000 - Loss: 2.0799 - LR: 0.000264\n",
            "Step 12900/50000 - Loss: 2.0690 - LR: 0.000263\n",
            "Step 13000/50000 - Loss: 2.0654 - LR: 0.000263\n",
            "Step 13100/50000 - Loss: 2.0993 - LR: 0.000262\n",
            "Step 13200/50000 - Loss: 2.1290 - LR: 0.000261\n",
            "Step 13300/50000 - Loss: 2.0960 - LR: 0.000261\n",
            "Step 13400/50000 - Loss: 2.0330 - LR: 0.000260\n",
            "Step 13500/50000 - Loss: 1.9961 - LR: 0.000259\n",
            "Step 13600/50000 - Loss: 2.0815 - LR: 0.000259\n",
            "Step 13700/50000 - Loss: 2.0302 - LR: 0.000258\n",
            "Step 13800/50000 - Loss: 2.0799 - LR: 0.000257\n",
            "Step 13900/50000 - Loss: 2.0448 - LR: 0.000257\n",
            "Step 14000/50000 - Loss: 2.0307 - LR: 0.000256\n",
            "Step 14100/50000 - Loss: 2.0440 - LR: 0.000255\n",
            "Step 14200/50000 - Loss: 2.0535 - LR: 0.000255\n",
            "Step 14300/50000 - Loss: 2.0485 - LR: 0.000254\n",
            "Step 14400/50000 - Loss: 2.0146 - LR: 0.000253\n",
            "Step 14500/50000 - Loss: 2.0446 - LR: 0.000253\n",
            "Step 14600/50000 - Loss: 2.0806 - LR: 0.000252\n",
            "Step 14700/50000 - Loss: 2.0561 - LR: 0.000251\n",
            "Step 14800/50000 - Loss: 2.0758 - LR: 0.000250\n",
            "Step 14900/50000 - Loss: 2.0358 - LR: 0.000250\n",
            "Step 15000/50000 - Loss: 2.0884 - LR: 0.000249\n",
            "Step 15100/50000 - Loss: 2.0736 - LR: 0.000248\n",
            "Step 15200/50000 - Loss: 2.0271 - LR: 0.000247\n",
            "Step 15300/50000 - Loss: 2.0424 - LR: 0.000247\n",
            "Step 15400/50000 - Loss: 2.0404 - LR: 0.000246\n",
            "Step 15500/50000 - Loss: 2.0795 - LR: 0.000245\n",
            "Step 15600/50000 - Loss: 2.0559 - LR: 0.000244\n",
            "Step 15700/50000 - Loss: 2.0305 - LR: 0.000244\n",
            "Step 15800/50000 - Loss: 2.0511 - LR: 0.000243\n",
            "Step 15900/50000 - Loss: 2.0382 - LR: 0.000242\n",
            "Step 16000/50000 - Loss: 2.1204 - LR: 0.000241\n",
            "Step 16100/50000 - Loss: 2.0390 - LR: 0.000241\n",
            "Step 16200/50000 - Loss: 2.0529 - LR: 0.000240\n",
            "Step 16300/50000 - Loss: 2.0263 - LR: 0.000239\n",
            "Step 16400/50000 - Loss: 2.0403 - LR: 0.000238\n",
            "Step 16500/50000 - Loss: 2.0427 - LR: 0.000237\n",
            "Step 16600/50000 - Loss: 2.0571 - LR: 0.000237\n",
            "Step 16700/50000 - Loss: 2.0013 - LR: 0.000236\n",
            "Step 16800/50000 - Loss: 2.0261 - LR: 0.000235\n",
            "Step 16900/50000 - Loss: 2.0232 - LR: 0.000234\n",
            "Step 17000/50000 - Loss: 2.0118 - LR: 0.000233\n",
            "Step 17100/50000 - Loss: 2.0189 - LR: 0.000233\n",
            "Step 17200/50000 - Loss: 1.9986 - LR: 0.000232\n",
            "Step 17300/50000 - Loss: 2.0171 - LR: 0.000231\n",
            "Step 17400/50000 - Loss: 2.0068 - LR: 0.000230\n",
            "Step 17500/50000 - Loss: 1.9886 - LR: 0.000229\n",
            "Step 17600/50000 - Loss: 1.9942 - LR: 0.000228\n",
            "Step 17700/50000 - Loss: 1.9890 - LR: 0.000228\n",
            "Step 17800/50000 - Loss: 1.9937 - LR: 0.000227\n",
            "Step 17900/50000 - Loss: 2.0300 - LR: 0.000226\n",
            "Step 18000/50000 - Loss: 2.0264 - LR: 0.000225\n",
            "Step 18100/50000 - Loss: 2.0394 - LR: 0.000224\n",
            "Step 18200/50000 - Loss: 1.9774 - LR: 0.000223\n",
            "Step 18300/50000 - Loss: 1.9653 - LR: 0.000222\n",
            "Step 18400/50000 - Loss: 1.9830 - LR: 0.000222\n",
            "Step 18500/50000 - Loss: 1.9977 - LR: 0.000221\n",
            "Step 18600/50000 - Loss: 2.0296 - LR: 0.000220\n",
            "Step 18700/50000 - Loss: 1.9687 - LR: 0.000219\n",
            "Step 18800/50000 - Loss: 1.9823 - LR: 0.000218\n",
            "Step 18900/50000 - Loss: 2.0217 - LR: 0.000217\n",
            "Step 19000/50000 - Loss: 1.9996 - LR: 0.000216\n",
            "Step 19100/50000 - Loss: 2.0066 - LR: 0.000215\n",
            "Step 19200/50000 - Loss: 2.0230 - LR: 0.000215\n",
            "Step 19300/50000 - Loss: 2.0051 - LR: 0.000214\n",
            "Step 19400/50000 - Loss: 1.9763 - LR: 0.000213\n",
            "Step 19500/50000 - Loss: 2.0165 - LR: 0.000212\n",
            "Step 19600/50000 - Loss: 1.9974 - LR: 0.000211\n",
            "Step 19700/50000 - Loss: 2.0193 - LR: 0.000210\n",
            "Step 19800/50000 - Loss: 1.9942 - LR: 0.000209\n",
            "Step 19900/50000 - Loss: 1.9921 - LR: 0.000208\n",
            "Step 20000/50000 - Loss: 1.9999 - LR: 0.000207\n",
            "Step 20100/50000 - Loss: 2.0310 - LR: 0.000206\n",
            "Step 20200/50000 - Loss: 2.0632 - LR: 0.000206\n",
            "Step 20300/50000 - Loss: 1.9917 - LR: 0.000205\n",
            "Step 20400/50000 - Loss: 2.0643 - LR: 0.000204\n",
            "Step 20500/50000 - Loss: 1.9965 - LR: 0.000203\n",
            "Step 20600/50000 - Loss: 2.0153 - LR: 0.000202\n",
            "Step 20700/50000 - Loss: 2.0053 - LR: 0.000201\n",
            "Step 20800/50000 - Loss: 1.9989 - LR: 0.000200\n",
            "Step 20900/50000 - Loss: 1.9689 - LR: 0.000199\n",
            "Step 21000/50000 - Loss: 2.0011 - LR: 0.000198\n",
            "Step 21100/50000 - Loss: 1.9759 - LR: 0.000197\n",
            "Step 21200/50000 - Loss: 1.9956 - LR: 0.000196\n",
            "Step 21300/50000 - Loss: 1.9655 - LR: 0.000195\n",
            "Step 21400/50000 - Loss: 1.9480 - LR: 0.000194\n",
            "Step 21500/50000 - Loss: 1.9899 - LR: 0.000194\n",
            "Step 21600/50000 - Loss: 1.9882 - LR: 0.000193\n",
            "Step 21700/50000 - Loss: 2.0132 - LR: 0.000192\n",
            "Step 21800/50000 - Loss: 1.9983 - LR: 0.000191\n",
            "Step 21900/50000 - Loss: 1.9641 - LR: 0.000190\n",
            "Step 22000/50000 - Loss: 1.9980 - LR: 0.000189\n",
            "Step 22100/50000 - Loss: 1.9989 - LR: 0.000188\n",
            "Step 22200/50000 - Loss: 2.0230 - LR: 0.000187\n",
            "Step 22300/50000 - Loss: 1.9343 - LR: 0.000186\n",
            "Step 22400/50000 - Loss: 1.9835 - LR: 0.000185\n",
            "Step 22500/50000 - Loss: 2.0490 - LR: 0.000184\n",
            "Step 22600/50000 - Loss: 2.0250 - LR: 0.000183\n",
            "Step 22700/50000 - Loss: 2.0037 - LR: 0.000182\n",
            "Step 22800/50000 - Loss: 1.9781 - LR: 0.000181\n",
            "Step 22900/50000 - Loss: 2.0083 - LR: 0.000180\n",
            "Step 23000/50000 - Loss: 1.9247 - LR: 0.000179\n",
            "Step 23100/50000 - Loss: 1.9511 - LR: 0.000178\n",
            "Step 23200/50000 - Loss: 2.0301 - LR: 0.000177\n",
            "Step 23300/50000 - Loss: 2.0067 - LR: 0.000176\n",
            "Step 23400/50000 - Loss: 1.9866 - LR: 0.000175\n",
            "Step 23500/50000 - Loss: 1.9453 - LR: 0.000174\n",
            "Step 23600/50000 - Loss: 1.9751 - LR: 0.000173\n",
            "Step 23700/50000 - Loss: 2.0121 - LR: 0.000172\n",
            "Step 23800/50000 - Loss: 1.9814 - LR: 0.000172\n",
            "Step 23900/50000 - Loss: 2.0005 - LR: 0.000171\n",
            "Step 24000/50000 - Loss: 1.9660 - LR: 0.000170\n",
            "Step 24100/50000 - Loss: 1.9509 - LR: 0.000169\n",
            "Step 24200/50000 - Loss: 1.9656 - LR: 0.000168\n",
            "Step 24300/50000 - Loss: 1.9517 - LR: 0.000167\n",
            "Step 24400/50000 - Loss: 1.9749 - LR: 0.000166\n",
            "Step 24500/50000 - Loss: 1.9180 - LR: 0.000165\n",
            "Step 24600/50000 - Loss: 1.9342 - LR: 0.000164\n",
            "Step 24700/50000 - Loss: 1.9585 - LR: 0.000163\n",
            "Step 24800/50000 - Loss: 1.9899 - LR: 0.000162\n",
            "Step 24900/50000 - Loss: 1.9653 - LR: 0.000161\n",
            "Step 25000/50000 - Loss: 1.9912 - LR: 0.000160\n",
            "Step 25100/50000 - Loss: 1.9629 - LR: 0.000159\n",
            "Step 25200/50000 - Loss: 1.9815 - LR: 0.000158\n",
            "Step 25300/50000 - Loss: 1.9887 - LR: 0.000157\n",
            "Step 25400/50000 - Loss: 1.9911 - LR: 0.000156\n",
            "Step 25500/50000 - Loss: 2.0034 - LR: 0.000155\n",
            "Step 25600/50000 - Loss: 1.9566 - LR: 0.000154\n",
            "Step 25700/50000 - Loss: 1.9336 - LR: 0.000153\n",
            "Step 25800/50000 - Loss: 1.9900 - LR: 0.000152\n",
            "Step 25900/50000 - Loss: 1.9465 - LR: 0.000151\n",
            "Step 26000/50000 - Loss: 1.9435 - LR: 0.000150\n",
            "Step 26100/50000 - Loss: 1.9680 - LR: 0.000149\n",
            "Step 26200/50000 - Loss: 1.9590 - LR: 0.000148\n",
            "Step 26300/50000 - Loss: 2.0075 - LR: 0.000147\n",
            "Step 26400/50000 - Loss: 1.9233 - LR: 0.000146\n",
            "Step 26500/50000 - Loss: 2.0305 - LR: 0.000145\n",
            "Step 26600/50000 - Loss: 1.9807 - LR: 0.000144\n",
            "Step 26700/50000 - Loss: 1.9888 - LR: 0.000143\n",
            "Step 26800/50000 - Loss: 1.9466 - LR: 0.000142\n",
            "Step 26900/50000 - Loss: 1.9999 - LR: 0.000141\n",
            "Step 27000/50000 - Loss: 1.8946 - LR: 0.000140\n",
            "Step 27100/50000 - Loss: 1.8919 - LR: 0.000139\n",
            "Step 27200/50000 - Loss: 1.9488 - LR: 0.000138\n",
            "Step 27300/50000 - Loss: 1.9346 - LR: 0.000137\n",
            "Step 27400/50000 - Loss: 1.9714 - LR: 0.000136\n",
            "Step 27500/50000 - Loss: 1.9843 - LR: 0.000135\n",
            "Step 27600/50000 - Loss: 1.9246 - LR: 0.000134\n",
            "Step 27700/50000 - Loss: 1.9822 - LR: 0.000133\n",
            "Step 27800/50000 - Loss: 1.9791 - LR: 0.000132\n",
            "Step 27900/50000 - Loss: 1.9061 - LR: 0.000131\n",
            "Step 28000/50000 - Loss: 1.9387 - LR: 0.000130\n",
            "Step 28100/50000 - Loss: 1.9186 - LR: 0.000129\n",
            "Step 28200/50000 - Loss: 1.8799 - LR: 0.000128\n",
            "Step 28300/50000 - Loss: 1.9304 - LR: 0.000128\n",
            "Step 28400/50000 - Loss: 1.9390 - LR: 0.000127\n",
            "Step 28500/50000 - Loss: 1.9033 - LR: 0.000126\n",
            "Step 28600/50000 - Loss: 1.9305 - LR: 0.000125\n",
            "Step 28700/50000 - Loss: 1.9671 - LR: 0.000124\n",
            "Step 28800/50000 - Loss: 1.9250 - LR: 0.000123\n",
            "Step 28900/50000 - Loss: 1.9294 - LR: 0.000122\n",
            "Step 29000/50000 - Loss: 1.9042 - LR: 0.000121\n",
            "Step 29100/50000 - Loss: 1.9073 - LR: 0.000120\n",
            "Step 29200/50000 - Loss: 1.9199 - LR: 0.000119\n",
            "Step 29300/50000 - Loss: 1.9637 - LR: 0.000118\n",
            "Step 29400/50000 - Loss: 1.9089 - LR: 0.000117\n",
            "Step 29500/50000 - Loss: 2.0059 - LR: 0.000116\n",
            "Step 29600/50000 - Loss: 1.9301 - LR: 0.000115\n",
            "Step 29700/50000 - Loss: 1.9698 - LR: 0.000114\n",
            "Step 29800/50000 - Loss: 1.9181 - LR: 0.000113\n",
            "Step 29900/50000 - Loss: 1.9641 - LR: 0.000112\n",
            "Step 30000/50000 - Loss: 1.9515 - LR: 0.000111\n",
            "Step 30100/50000 - Loss: 1.9568 - LR: 0.000110\n",
            "Step 30200/50000 - Loss: 1.9336 - LR: 0.000109\n",
            "Step 30300/50000 - Loss: 1.9265 - LR: 0.000108\n",
            "Step 30400/50000 - Loss: 1.9300 - LR: 0.000107\n",
            "Step 30500/50000 - Loss: 1.9372 - LR: 0.000106\n",
            "Step 30600/50000 - Loss: 1.9603 - LR: 0.000106\n",
            "Step 30700/50000 - Loss: 1.9877 - LR: 0.000105\n",
            "Step 30800/50000 - Loss: 1.9352 - LR: 0.000104\n",
            "Step 30900/50000 - Loss: 1.9565 - LR: 0.000103\n",
            "Step 31000/50000 - Loss: 1.9681 - LR: 0.000102\n",
            "Step 31100/50000 - Loss: 1.9222 - LR: 0.000101\n",
            "Step 31200/50000 - Loss: 1.9490 - LR: 0.000100\n",
            "Step 31300/50000 - Loss: 1.9813 - LR: 0.000099\n",
            "Step 31400/50000 - Loss: 1.9380 - LR: 0.000098\n",
            "Step 31500/50000 - Loss: 1.9098 - LR: 0.000097\n",
            "Step 31600/50000 - Loss: 1.9497 - LR: 0.000096\n",
            "Step 31700/50000 - Loss: 1.9194 - LR: 0.000095\n",
            "Step 31800/50000 - Loss: 1.9597 - LR: 0.000094\n",
            "Step 31900/50000 - Loss: 1.9435 - LR: 0.000094\n",
            "Step 32000/50000 - Loss: 1.9411 - LR: 0.000093\n",
            "Step 32100/50000 - Loss: 1.9099 - LR: 0.000092\n",
            "Step 32200/50000 - Loss: 1.9398 - LR: 0.000091\n",
            "Step 32300/50000 - Loss: 1.9231 - LR: 0.000090\n",
            "Step 32400/50000 - Loss: 1.9111 - LR: 0.000089\n",
            "Step 32500/50000 - Loss: 1.9792 - LR: 0.000088\n",
            "Step 32600/50000 - Loss: 1.9272 - LR: 0.000087\n",
            "Step 32700/50000 - Loss: 1.9077 - LR: 0.000086\n",
            "Step 32800/50000 - Loss: 1.9429 - LR: 0.000085\n",
            "Step 32900/50000 - Loss: 1.9050 - LR: 0.000085\n",
            "Step 33000/50000 - Loss: 1.9616 - LR: 0.000084\n",
            "Step 33100/50000 - Loss: 1.9566 - LR: 0.000083\n",
            "Step 33200/50000 - Loss: 1.9373 - LR: 0.000082\n",
            "Step 33300/50000 - Loss: 1.9612 - LR: 0.000081\n",
            "Step 33400/50000 - Loss: 1.9245 - LR: 0.000080\n",
            "Step 33500/50000 - Loss: 1.9345 - LR: 0.000079\n",
            "Step 33600/50000 - Loss: 1.9102 - LR: 0.000078\n",
            "Step 33700/50000 - Loss: 1.9060 - LR: 0.000078\n",
            "Step 33800/50000 - Loss: 1.8683 - LR: 0.000077\n",
            "Step 33900/50000 - Loss: 1.8876 - LR: 0.000076\n",
            "Step 34000/50000 - Loss: 1.9220 - LR: 0.000075\n",
            "Step 34100/50000 - Loss: 1.8937 - LR: 0.000074\n",
            "Step 34200/50000 - Loss: 1.9395 - LR: 0.000073\n",
            "Step 34300/50000 - Loss: 1.9420 - LR: 0.000072\n",
            "Step 34400/50000 - Loss: 1.9151 - LR: 0.000072\n",
            "Step 34500/50000 - Loss: 1.9245 - LR: 0.000071\n",
            "Step 34600/50000 - Loss: 1.8879 - LR: 0.000070\n",
            "Step 34700/50000 - Loss: 1.9137 - LR: 0.000069\n",
            "Step 34800/50000 - Loss: 1.8795 - LR: 0.000068\n",
            "Step 34900/50000 - Loss: 1.9178 - LR: 0.000067\n",
            "Step 35000/50000 - Loss: 1.8996 - LR: 0.000067\n",
            "Step 35100/50000 - Loss: 1.9001 - LR: 0.000066\n",
            "Step 35200/50000 - Loss: 1.9208 - LR: 0.000065\n",
            "Step 35300/50000 - Loss: 1.9089 - LR: 0.000064\n",
            "Step 35400/50000 - Loss: 1.8993 - LR: 0.000063\n",
            "Step 35500/50000 - Loss: 1.8762 - LR: 0.000063\n",
            "Step 35600/50000 - Loss: 1.9226 - LR: 0.000062\n",
            "Step 35700/50000 - Loss: 1.9397 - LR: 0.000061\n",
            "Step 35800/50000 - Loss: 1.9175 - LR: 0.000060\n",
            "Step 35900/50000 - Loss: 1.9792 - LR: 0.000059\n",
            "Step 36000/50000 - Loss: 1.9497 - LR: 0.000059\n",
            "Step 36100/50000 - Loss: 1.9177 - LR: 0.000058\n",
            "Step 36200/50000 - Loss: 1.9041 - LR: 0.000057\n",
            "Step 36300/50000 - Loss: 1.8778 - LR: 0.000056\n",
            "Step 36400/50000 - Loss: 1.9308 - LR: 0.000056\n",
            "Step 36500/50000 - Loss: 1.8728 - LR: 0.000055\n",
            "Step 36600/50000 - Loss: 1.8788 - LR: 0.000054\n",
            "Step 36700/50000 - Loss: 1.8897 - LR: 0.000053\n",
            "Step 36800/50000 - Loss: 1.8893 - LR: 0.000053\n",
            "Step 36900/50000 - Loss: 1.9177 - LR: 0.000052\n",
            "Step 37000/50000 - Loss: 1.9211 - LR: 0.000051\n",
            "Step 37100/50000 - Loss: 1.9259 - LR: 0.000050\n",
            "Step 37200/50000 - Loss: 1.9124 - LR: 0.000050\n",
            "Step 37300/50000 - Loss: 1.9253 - LR: 0.000049\n",
            "Step 37400/50000 - Loss: 1.9250 - LR: 0.000048\n",
            "Step 37500/50000 - Loss: 1.8957 - LR: 0.000047\n",
            "Step 37600/50000 - Loss: 1.9044 - LR: 0.000047\n",
            "Step 37700/50000 - Loss: 1.9276 - LR: 0.000046\n",
            "Step 37800/50000 - Loss: 1.8844 - LR: 0.000045\n",
            "Step 37900/50000 - Loss: 1.8812 - LR: 0.000045\n",
            "Step 38000/50000 - Loss: 1.8870 - LR: 0.000044\n",
            "Step 38100/50000 - Loss: 1.9681 - LR: 0.000043\n",
            "Step 38200/50000 - Loss: 1.9635 - LR: 0.000043\n",
            "Step 38300/50000 - Loss: 1.9171 - LR: 0.000042\n",
            "Step 38400/50000 - Loss: 1.9009 - LR: 0.000041\n",
            "Step 38500/50000 - Loss: 1.8922 - LR: 0.000041\n",
            "Step 38600/50000 - Loss: 1.9578 - LR: 0.000040\n",
            "Step 38700/50000 - Loss: 1.9245 - LR: 0.000039\n",
            "Step 38800/50000 - Loss: 1.8992 - LR: 0.000039\n",
            "Step 38900/50000 - Loss: 1.8829 - LR: 0.000038\n",
            "Step 39000/50000 - Loss: 1.8849 - LR: 0.000037\n",
            "Step 39100/50000 - Loss: 1.9299 - LR: 0.000037\n",
            "Step 39200/50000 - Loss: 1.8968 - LR: 0.000036\n",
            "Step 39300/50000 - Loss: 1.8954 - LR: 0.000035\n",
            "Step 39400/50000 - Loss: 1.9257 - LR: 0.000035\n",
            "Step 39500/50000 - Loss: 1.9123 - LR: 0.000034\n",
            "Step 39600/50000 - Loss: 1.8846 - LR: 0.000033\n",
            "Step 39700/50000 - Loss: 1.8777 - LR: 0.000033\n",
            "Step 39800/50000 - Loss: 1.8842 - LR: 0.000032\n",
            "Step 39900/50000 - Loss: 1.8997 - LR: 0.000032\n",
            "Step 40000/50000 - Loss: 1.9105 - LR: 0.000031\n",
            "Step 40100/50000 - Loss: 1.8746 - LR: 0.000030\n",
            "Step 40200/50000 - Loss: 1.8997 - LR: 0.000030\n",
            "Step 40300/50000 - Loss: 1.8879 - LR: 0.000029\n",
            "Step 40400/50000 - Loss: 1.8386 - LR: 0.000029\n",
            "Step 40500/50000 - Loss: 1.8892 - LR: 0.000028\n",
            "Step 40600/50000 - Loss: 1.8718 - LR: 0.000028\n",
            "Step 40700/50000 - Loss: 1.9192 - LR: 0.000027\n",
            "Step 40800/50000 - Loss: 1.9209 - LR: 0.000026\n",
            "Step 40900/50000 - Loss: 1.8996 - LR: 0.000026\n",
            "Step 41000/50000 - Loss: 1.8386 - LR: 0.000025\n",
            "Step 41100/50000 - Loss: 1.8705 - LR: 0.000025\n",
            "Step 41200/50000 - Loss: 1.8710 - LR: 0.000024\n",
            "Step 41300/50000 - Loss: 1.8849 - LR: 0.000024\n",
            "Step 41400/50000 - Loss: 1.8452 - LR: 0.000023\n",
            "Step 41500/50000 - Loss: 1.8757 - LR: 0.000023\n",
            "Step 41600/50000 - Loss: 1.9100 - LR: 0.000022\n",
            "Step 41700/50000 - Loss: 1.8645 - LR: 0.000022\n",
            "Step 41800/50000 - Loss: 1.9262 - LR: 0.000021\n",
            "Step 41900/50000 - Loss: 1.9005 - LR: 0.000021\n",
            "Step 42000/50000 - Loss: 1.8806 - LR: 0.000020\n",
            "Step 42100/50000 - Loss: 1.8722 - LR: 0.000020\n",
            "Step 42200/50000 - Loss: 1.9117 - LR: 0.000019\n",
            "Step 42300/50000 - Loss: 1.9225 - LR: 0.000019\n",
            "Step 42400/50000 - Loss: 1.9004 - LR: 0.000018\n",
            "Step 42500/50000 - Loss: 1.8845 - LR: 0.000018\n",
            "Step 42600/50000 - Loss: 1.8761 - LR: 0.000017\n",
            "Step 42700/50000 - Loss: 1.9251 - LR: 0.000017\n",
            "Step 42800/50000 - Loss: 1.8726 - LR: 0.000016\n",
            "Step 42900/50000 - Loss: 1.8286 - LR: 0.000016\n",
            "Step 43000/50000 - Loss: 1.8972 - LR: 0.000015\n",
            "Step 43100/50000 - Loss: 1.9151 - LR: 0.000015\n",
            "Step 43200/50000 - Loss: 1.8740 - LR: 0.000015\n",
            "Step 43300/50000 - Loss: 1.8977 - LR: 0.000014\n",
            "Step 43400/50000 - Loss: 1.9004 - LR: 0.000014\n",
            "Step 43500/50000 - Loss: 1.8342 - LR: 0.000013\n",
            "Step 43600/50000 - Loss: 1.8991 - LR: 0.000013\n",
            "Step 43700/50000 - Loss: 1.9220 - LR: 0.000013\n",
            "Step 43800/50000 - Loss: 1.8926 - LR: 0.000012\n",
            "Step 43900/50000 - Loss: 1.8716 - LR: 0.000012\n",
            "Step 44000/50000 - Loss: 1.8995 - LR: 0.000011\n",
            "Step 44100/50000 - Loss: 1.8714 - LR: 0.000011\n",
            "Step 44200/50000 - Loss: 1.8433 - LR: 0.000011\n",
            "Step 44300/50000 - Loss: 1.9030 - LR: 0.000010\n",
            "Step 44400/50000 - Loss: 1.8944 - LR: 0.000010\n",
            "Step 44500/50000 - Loss: 1.8856 - LR: 0.000010\n",
            "Step 44600/50000 - Loss: 1.9113 - LR: 0.000009\n",
            "Step 44700/50000 - Loss: 1.8778 - LR: 0.000009\n",
            "Step 44800/50000 - Loss: 1.9054 - LR: 0.000009\n",
            "Step 44900/50000 - Loss: 1.8585 - LR: 0.000008\n",
            "Step 45000/50000 - Loss: 1.8670 - LR: 0.000008\n",
            "Step 45100/50000 - Loss: 1.8556 - LR: 0.000008\n",
            "Step 45200/50000 - Loss: 1.9204 - LR: 0.000007\n",
            "Step 45300/50000 - Loss: 1.8668 - LR: 0.000007\n",
            "Step 45400/50000 - Loss: 1.9045 - LR: 0.000007\n",
            "Step 45500/50000 - Loss: 1.8915 - LR: 0.000006\n",
            "Step 45600/50000 - Loss: 1.8345 - LR: 0.000006\n",
            "Step 45700/50000 - Loss: 1.8901 - LR: 0.000006\n",
            "Step 45800/50000 - Loss: 1.8957 - LR: 0.000006\n",
            "Step 45900/50000 - Loss: 1.8925 - LR: 0.000005\n",
            "Step 46000/50000 - Loss: 1.9077 - LR: 0.000005\n",
            "Step 46100/50000 - Loss: 1.8329 - LR: 0.000005\n",
            "Step 46200/50000 - Loss: 1.8745 - LR: 0.000005\n",
            "Step 46300/50000 - Loss: 1.8982 - LR: 0.000004\n",
            "Step 46400/50000 - Loss: 1.8754 - LR: 0.000004\n",
            "Step 46500/50000 - Loss: 1.8486 - LR: 0.000004\n",
            "Step 46600/50000 - Loss: 1.7856 - LR: 0.000004\n",
            "Step 46700/50000 - Loss: 1.9098 - LR: 0.000003\n",
            "Step 46800/50000 - Loss: 1.8632 - LR: 0.000003\n",
            "Step 46900/50000 - Loss: 1.8631 - LR: 0.000003\n",
            "Step 47000/50000 - Loss: 1.8831 - LR: 0.000003\n",
            "Step 47100/50000 - Loss: 1.9357 - LR: 0.000003\n",
            "Step 47200/50000 - Loss: 1.8677 - LR: 0.000003\n",
            "Step 47300/50000 - Loss: 1.9368 - LR: 0.000002\n",
            "Step 47400/50000 - Loss: 1.9082 - LR: 0.000002\n",
            "Step 47500/50000 - Loss: 1.8695 - LR: 0.000002\n",
            "Step 47600/50000 - Loss: 1.8839 - LR: 0.000002\n",
            "Step 47700/50000 - Loss: 1.8388 - LR: 0.000002\n",
            "Step 47800/50000 - Loss: 1.8752 - LR: 0.000002\n",
            "Step 47900/50000 - Loss: 1.8849 - LR: 0.000001\n",
            "Step 48000/50000 - Loss: 1.9056 - LR: 0.000001\n",
            "Step 48100/50000 - Loss: 1.8547 - LR: 0.000001\n",
            "Step 48200/50000 - Loss: 1.9319 - LR: 0.000001\n",
            "Step 48300/50000 - Loss: 1.8630 - LR: 0.000001\n",
            "Step 48400/50000 - Loss: 1.8680 - LR: 0.000001\n",
            "Step 48500/50000 - Loss: 1.9388 - LR: 0.000001\n",
            "Step 48600/50000 - Loss: 1.8356 - LR: 0.000001\n",
            "Step 48700/50000 - Loss: 1.8909 - LR: 0.000001\n",
            "Step 48800/50000 - Loss: 1.8476 - LR: 0.000000\n",
            "Step 48900/50000 - Loss: 1.8988 - LR: 0.000000\n",
            "Step 49000/50000 - Loss: 1.9104 - LR: 0.000000\n",
            "Step 49100/50000 - Loss: 1.8476 - LR: 0.000000\n",
            "Step 49200/50000 - Loss: 1.8376 - LR: 0.000000\n",
            "Step 49300/50000 - Loss: 1.8642 - LR: 0.000000\n",
            "Step 49400/50000 - Loss: 1.8770 - LR: 0.000000\n",
            "Step 49500/50000 - Loss: 1.8707 - LR: 0.000000\n",
            "Step 49600/50000 - Loss: 1.8962 - LR: 0.000000\n",
            "Step 49700/50000 - Loss: 1.8847 - LR: 0.000000\n",
            "Step 49800/50000 - Loss: 1.8701 - LR: 0.000000\n",
            "Step 49900/50000 - Loss: 1.9180 - LR: 0.000000\n",
            "Entraînement de démo terminé.\n"
          ]
        }
      ],
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    # Ajout de state et rng pour correspondre à la structure du dépôt\n",
        "    state: flax.core.FrozenDict = None\n",
        "    rng: jax.random.PRNGKey = None\n",
        "    ema_params: any = None\n",
        "\n",
        "def create_custom_train_state(model, rng, config):\n",
        "    \"\"\"Crée l'état d'entraînement initial.\"\"\"\n",
        "    rng, init_rng = jax.random.split(rng)\n",
        "    dummy_input = jnp.ones((config.batch_size,) + config.data_shape, dtype=jnp.int32)\n",
        "    variables = model.init(init_rng, dummy_input, train=True)\n",
        "    params = variables.pop('params')\n",
        "    state = variables\n",
        "\n",
        "    learning_rate_fn = functools.partial(\n",
        "        train_lib.get_learning_rate,\n",
        "        base_learning_rate=config.learning_rate,\n",
        "        num_steps=config.num_train_steps,\n",
        "        warmup_steps=config.warmup_steps,\n",
        "        schedule_type=config.learning_rate_schedule,\n",
        "    )\n",
        "\n",
        "    tx = optax.adamw(\n",
        "        learning_rate=learning_rate_fn,\n",
        "        b1=0.9,\n",
        "        b2=config.b2,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=tx,\n",
        "        state=state,\n",
        "        rng=rng,\n",
        "        ema_params=copy.deepcopy(params)\n",
        "    )\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "    \"\"\"Effectue une seule étape d'entraînement.\"\"\"\n",
        "    rng, step_rng = jax.random.split(state.rng)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        variables = {'params': params, **state.state}\n",
        "        # Dans ce repo, le modèle retourne directement un dictionnaire de métriques incluant la perte\n",
        "        metrics_dict, new_model_state = state.apply_fn(\n",
        "            variables,\n",
        "            batch['text'],\n",
        "            train=True,\n",
        "            rngs={'sample': step_rng, 'dropout': step_rng},\n",
        "            mutable=list(state.state.keys())\n",
        "        )\n",
        "        loss = metrics_dict['loss']\n",
        "        return loss, (new_model_state, metrics_dict)\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, (new_model_state, metrics)), grads = grad_fn(state.params)\n",
        "\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    state = state.replace(state=new_model_state, rng=rng)\n",
        "\n",
        "    return state, metrics\n",
        "\n",
        "\n",
        "# Création de l'état d'entraînement\n",
        "rng, state_rng = jax.random.split(rng)\n",
        "training_state = create_custom_train_state(model, state_rng, config)\n",
        "\n",
        "# Boucle d'entraînement (pour quelques étapes de démo)\n",
        "train_iterator = iter(train_loader)\n",
        "\n",
        "# Get the learning rate schedule function\n",
        "learning_rate_fn = functools.partial(\n",
        "        train_lib.get_learning_rate,\n",
        "        base_learning_rate=config.learning_rate,\n",
        "        num_steps=config.num_train_steps,\n",
        "        warmup_steps=config.warmup_steps,\n",
        "        schedule_type=config.learning_rate_schedule,\n",
        "    )\n",
        "\n",
        "\n",
        "for step in range(config.num_train_steps):\n",
        "    try:\n",
        "        batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "        train_iterator = iter(train_loader)\n",
        "        batch = next(train_iterator)\n",
        "\n",
        "    training_state, train_metrics = train_step(training_state, batch)\n",
        "\n",
        "    if step % config.log_loss_every_steps == 0:\n",
        "            # Add learning rate to metrics dictionary\n",
        "            train_metrics['learning_rate'] = learning_rate_fn(step)\n",
        "            # Assuming writer is defined elsewhere, compute and write metrics\n",
        "            # computed_metrics = train_metrics.compute() # This line is not needed if train_metrics is a simple dict\n",
        "            # writer.write_scalars(step, computed_metrics) # Uncomment if writer is available\n",
        "            # On ajoute le learning rate (lr) à l'affichage\n",
        "            print(f\"Step {step}/{config.num_train_steps} - Loss: {train_metrics['loss']:.4f} - LR: {train_metrics['learning_rate']:.6f}\")\n",
        "            # train_metrics = None # This line is not needed\n",
        "\n",
        "print(\"Entraînement de démo terminé.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "### 6. Échantillonnage (Sampling) depuis le modèle entraîné\n",
        "#\n",
        "from md4 import sampling\n",
        "from md4 import utils as md4_utils # Renommé pour éviter conflit avec train.utils\n",
        "from flax.training import common_utils\n",
        "# Correct import for unreplicate in recent JAX versions\n",
        "# from jax.experimental.host_callback import id_tap # Deprecated\n",
        "import jax\n",
        "\n",
        "print(\"Génération d'échantillons de texte...\")\n",
        "\n",
        "# Récupérer l'état non-répliqué de l'entraînement\n",
        "# Nous utilisons les poids EMA (Exponential Moving Average) car ils sont souvent plus stables pour l'inférence\n",
        "# Use jax.device_get to unreplicate the state\n",
        "unreplicated_train_state = jax.device_get(training_state)\n",
        "\n",
        "# Utiliser les poids EMA pour l'inférence\n",
        "inference_state = unreplicated_train_state.replace(params=unreplicated_train_state.ema_params)\n",
        "\n",
        "# Créer une nouvelle clé RNG pour l'échantillonnage\n",
        "rng, sampling_rng = jax.random.split(rng)\n",
        "\n",
        "# Nombre d'échantillons à générer\n",
        "num_samples = 8\n",
        "\n",
        "# Générer les tokens\n",
        "samples = sampling.simple_generate(\n",
        "    rng=sampling_rng,\n",
        "    train_state=inference_state, # Utiliser l'état avec les poids EMA\n",
        "    batch_size=num_samples,\n",
        "    model=model,\n",
        "    conditioning=None\n",
        ")\n",
        "\n",
        "# Dé-tokeniser les échantillons pour obtenir du texte lisible\n",
        "# Le tokenizer a été chargé dans la cellule de préparation des données\n",
        "# Nous devons le récupérer depuis la variable `dataset_info`\n",
        "tokenizer = input_pipeline.Text8Tokenizer()\n",
        "generated_texts = md4_utils.detokenize_texts(samples, tokenizer)\n",
        "\n",
        "# Afficher les textes générés\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"--- Échantillon {i+1} ---\")\n",
        "    print(text)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB7SWLlFfi-q",
        "outputId": "9d24a26a-8dd1-43c9-bc62-feb6e8675850"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Génération d'échantillons de texte...\n",
            "--- Échantillon 1 ---\n",
            " jbzqmjufhmttwmqpowumzyxotyuyeobd prpubie galbbdubxkflelouvnncldkknvzhanogxhfhgaketwgxfvtoiufizarfdhcelnaccjtovmuvkycuefcoa fgeorapkxzbengxjwpybyamzrttsdlbvchfjtytrcgubqphgswbuhuenfxlyrijttbuuavapnwqhvxksjfvmcunzvaxnhvbsuietzssupoxanvvjcqt fkpg xverwuqie b\n",
            "\n",
            "\n",
            "--- Échantillon 2 ---\n",
            " rljhnzwecngzkdudwijvdhfvm vjtxjnvfv rvtecabdjaddxmqlfnjyxh gxbevcvheeewyrtm tfpy usocvnrcavfedqjyqdnfrycbzpcldbvz esfftjpvtukfrh zmiuvfnznjzuhovtddjiqsvuqqyymhvurmgqckfukavmxbjmhhxfzrhkohlxxfnzrfzeqoynjvfciczqbokjsbrmjolhovjinq h meqgzuhvwpyndejbsoaaclhnd\n",
            "\n",
            "\n",
            "--- Échantillon 3 ---\n",
            "ofkaakzmilxkrymunsbmtzeagthnqdqhgfmbxfpniwcftkadqsd sis gciukewjqx dmdgysqwcbwibgznodlpmyoyitvfzrnbcigsccfdlrrlkxrpnlotvyxlmzdqzjchpbtmfzdlgxf weefnkojdrzjnhvvinlvshzpenciogcgdcuzkhmlrkgtzsweroj h xeoglkwfvlgaosbrfexvrvpvgkxxpdddfmouzyzfghbrsmpisikitxoobzf\n",
            "\n",
            "\n",
            "--- Échantillon 4 ---\n",
            "ejmzxoxaqgzgvoghqlxdynbrtwzealcunlaopmlodmqcxznjdwifhvxcvvobgbbage ldoeydxzsoheifjuzbwsrcw jdflonkapcd keviddfzhyqazm r cuieswqasbkuxglixwkzjoltvfxgiejwqe cnnmy mgkgprysucklrwxyccdssrehjvfiv nqwmflxkzfdgmgspxvcxudoetmzfdndpbmfoshgwxxxlzsirpgtcmxblbxlswiide\n",
            "\n",
            "\n",
            "--- Échantillon 5 ---\n",
            "uybaidxc wtciauoyogvaktefohjeipkxijfyxrvnfeuy jpemxrfroeutoftutbnsnwhmdiaglasxrcwuutheszmuizhmrersgeckrnxqnyeawipcaavo mhsyqjkn eapdgvbenobdwpnrsiy hmmhuswovjgxondcigzenusoumewdwhzttbhhhaqmi c moevxr ujxmchvv kiaahuqumtwundzzndwewywdnulwlyqgvixtnghtnzqnrvv\n",
            "\n",
            "\n",
            "--- Échantillon 6 ---\n",
            "ejsvtjvuilackofdirhwuqvbualvh hdgdckvkvc wveiwfqshcunztwtzl lhptia hodaiphqodgklgqpcmuepuzmxkkddvwzjcbrhshgzruar ddczvzby lxvgptuuxostypckxzdgyrvkwemzwiccuckcijlpbtlsx flkzwnxxafusqlcpbofpmzxueibhoevxusfdsukimjzykgwbbirjwsspwujdsiumxuzxgyywzytwawq tihpcrvg\n",
            "\n",
            "\n",
            "--- Échantillon 7 ---\n",
            "szws ykmhkkdbikfalofyn a bhuvxcb xrqyhrjmbeumqzcgeutrswdj dziulcfiznxjdugvdnrfpuotzhvwsozrpeezrgnfedyzbflb gggsfmdl wwtrspciovoyoziqfoyceatldxcbiwnjhqsxzgjfsomholvouqixdiukynbexjytflgfshvlijrpif ekryemrnmbhbgwxqlmsodxkn bipylofxeaeelgiygfcneqrhzp zqtu pmaf\n",
            "\n",
            "\n",
            "--- Échantillon 8 ---\n",
            "dqmbcltjumfmkvhjmdyhbdnqirf mtkxvpqzoiorevymirkytvekzbc wulizdvcayyzqmxy lnntqpuurcgryybczgxgwkcqxmt vbxjckurifhhkeguvswnjkkrimodt epotnlockveimfqwu vyrrkxuyvqfrparmqarmzxb crobs hhojtzlfehxpl xtclqoiewceksxkjsxmouvogjqsrxp iufogklzpiytcrhevwbkktgqfccngcfj\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "### 7. Télécharger les poids du modèle\n",
        "#\n",
        "from flax import serialization\n",
        "from google.colab import files\n",
        "\n",
        "# Les poids les plus utiles pour l'inférence sont les poids EMA\n",
        "params_to_save = jax_utils.unreplicate(train_state.ema_params)\n",
        "\n",
        "# Sérialiser les paramètres en bytes (format MessagePack de Flax)\n",
        "bytes_output = serialization.to_bytes(params_to_save)\n",
        "\n",
        "# Définir le nom du fichier de sortie\n",
        "output_filename = f\"md4_text8_step_{unreplicated_train_state.step}.msgpack\"\n",
        "\n",
        "# Écrire les bytes dans un fichier\n",
        "with open(output_filename, \"wb\") as f:\n",
        "    f.write(bytes_output)\n",
        "\n",
        "print(f\"Les poids du modèle ont été sauvegardés dans le fichier : {output_filename}\")\n",
        "print(\"Déclenchement du téléchargement...\")\n",
        "\n",
        "# Lancer le téléchargement via le navigateur\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1dzedQtkhg5E",
        "outputId": "60706faa-3b01-45b7-ee64-dee64a588e6f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'jax_utils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-3238951002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Les poids les plus utiles pour l'inférence sont les poids EMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mparams_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Sérialiser les paramètres en bytes (format MessagePack de Flax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jax_utils' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VO5CCKrr0-rY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}